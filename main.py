# main.py

import os
from dotenv import load_dotenv

from sentence_transformers import SentenceTransformer

from rag import RAGRetriever
from summarizer import Summarizer
from key_takeaway_generator import KeyTakeawayGenerator
from faq_generator import FAQGenerator
from practice_question_generator import parse_practice_questions
from weaviate_handler import WeaviateHandler

from langchain_groq import ChatGroq

# â”€â”€ 1) ENV & MODEL SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL   = os.getenv("GROQ_MODEL", "llama3-8b-8192")

# 1a) Local embedding model (cached)
embedder = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def embed_texts(texts: list[str]) -> list[list[float]]:
    # returns list of embedding vectors
    return embedder.encode(texts, convert_to_numpy=True).tolist()

# 1b) Groq LLM (for summarization, FAQs, practice questions)
llm = ChatGroq(
    groq_api_key=GROQ_API_KEY,
    model_name=GROQ_MODEL,
    temperature=0.7,
    max_tokens=1024,
)

# â”€â”€ 2) PDF â†’ CHUNKS â†’ EMBEDDINGS â†’ WEAVIATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Adjust max_pages as desired
from pdf_extraction import extract_text_as_documents
from chunking import chunk_texts

docs   = extract_text_as_documents("test1.pdf", max_pages=3)
chunks = chunk_texts(docs)
valid_chunks    = [c for c in chunks if c.page_content.strip()]
text_chunks     = [c.page_content for c in valid_chunks]
metadata_chunks = [c.metadata     for c in valid_chunks]

print(f"ğŸ“„ Embedding {len(text_chunks)} chunksâ€¦")
for i, txt in enumerate(text_chunks):
    print(f"  [{i}] {txt[:60].replace(chr(10),' ')}â€¦")

embeddings = embed_texts(text_chunks)
if not embeddings:
    print("âŒ Embedding failed.")
    exit()

weaviate_handler = WeaviateHandler(collection_name="LectureSlides")
weaviate_handler.insert_chunks(text_chunks, embeddings, metadata_chunks)

# â”€â”€ 3) RAG RETRIEVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
retriever = RAGRetriever(collection_name="LectureSlides", embedding_model=embedder)
query     = "Give a summary of the lecture content."
retrieved = retriever.retrieve(query, k=15)

# â”€â”€ 4) SUMMARIZATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
summarizer = Summarizer(llm=llm)
summary     = summarizer.summarize(retrieved, question=query)
print("\nğŸ“ Summary:\n", summary.strip())

# â”€â”€ 5) KEY TAKEAWAYS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ktg       = KeyTakeawayGenerator(llm=llm)
takeaways = ktg.generate_takeaways(retrieved)
print("\nğŸ“Œ Key Takeaways:\n", takeaways.strip())

# â”€â”€ 6) FAQS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
faqg = FAQGenerator(llm=llm)
faqs = faqg.generate_faqs_from_chunks(retrieved, max_faqs=14)
print("\nâ“ FAQs:")
for faq in faqs:
    print(f"Q: {faq['question']}\nA: {faq['answer']}\n")


# â”€â”€ 7) PRACTICE QUESTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

print("\nğŸ§  Practice Questions:")
for i, chunk in enumerate(valid_chunks):
    prompt = f"""
Generate 2 multiple-choice questions and 1 open-ended question from the following content. 

Each MCQ should have 4 options labeled A) to D), followed by the correct answer in the format: 'Correct answer: <your answer>'.

Each open-ended question should be preceded by '**Open-ended question:**'.

Content:
\"\"\"
{chunk.page_content}
\"\"\"
"""
    try:
        raw_output = llm.invoke(prompt)
        print(f"\nğŸ’¬ Raw LLM Output for Chunk {i+1}:\n{raw_output.content}\n")
        parsed_questions = parse_practice_questions(raw_output.content)
    except Exception as e:
        print(f"âŒ Failed to generate questions for chunk {i+1}: {e}")
        continue

    print(f"[Chunk {i+1}]")
    
    for q in parsed_questions:
        if "type" not in q:
            print("âš ï¸ Skipping malformed question:", q)
            continue
    
        if q["type"] == "mcq":
            print(f"Q: {q['question']}")
            for j, option in enumerate(q["options"]):
                print(f"  {chr(65 + j)}) {option}")
            print(f"âœ” Answer: {q['answer']}\n")
    
        elif q["type"] == "open":
            print(f"Q (Open-ended): {q['question'].strip()}\n")
    



# â”€â”€ 8) CLEAN UP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
retriever.close()
weaviate_handler.close()
